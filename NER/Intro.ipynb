{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Token Classification (HF): Load Dataset and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's load data from conll2003 dataset and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique labels are stored in `raw_datasets[\"train\"].features[\"ner_tags\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the tags available for each of the word pre-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Farm Minister Loyola    de Palacio  had earlier accused Fischler   at   an    EU farm ministers    ' meeting   of causing unjustified alarm through \" dangerous generalisation . \" \n",
      " B-MISC    O        O  B-PER I-PER   I-PER    O       O       O    B-PER    O    O B-ORG    O         O    O       O    O       O           O     O       O O         O              O O O \n",
      "    NNP  NNP      NNP    NNP   NNP     NNP  VBD     RBR     VBN      NNP   IN   DT    JJ   NN       NNS  POS      NN   IN     VBG          JJ    NN      IN \"        JJ             NN . \" \n",
      "   B-NP I-NP     I-NP   I-NP  I-NP    I-NP B-VP    I-VP    I-VP     B-NP B-PP B-NP  I-NP I-NP      I-NP B-NP    I-NP B-PP    B-VP      B-ADJP  B-NP    B-PP O      B-NP           I-NP O O \n"
     ]
    }
   ],
   "source": [
    "ner_tags = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "pos_tags = raw_datasets[\"train\"].features[\"pos_tags\"].feature.names\n",
    "chunk_tags = raw_datasets[\"train\"].features[\"chunk_tags\"].feature.names\n",
    "idx = 10\n",
    "sample = raw_datasets[\"train\"][idx]\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "line3 = \"\"\n",
    "line4 = \"\"\n",
    "for token, ner, pos_tag, chunk in zip(sample[\"tokens\"], sample[\"ner_tags\"], sample[\"pos_tags\"], sample[\"chunk_tags\"]):\n",
    "    # print(f\"{token:15s} {pos_tags.feature.int2str(pos_tag)}\")\n",
    "    full_ner = ner_tags[ner]\n",
    "    full_pos = pos_tags[pos_tag]\n",
    "    full_chunk = chunk_tags[chunk]\n",
    "    max_length = max(len(token), len(full_pos), len(full_chunk), len(full_ner))\n",
    "    line1 += f\"{token:>{max_length}s} \"\n",
    "    line2 += f\"{full_ner:>{max_length}s} \"\n",
    "    line3 += f\"{full_pos:>{max_length}s} \"\n",
    "    line4 += f\"{full_chunk:>{max_length}s} \"\n",
    "    \n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3)\n",
    "print(line4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we have pre-tokenized texts with their corresponding labels. However, we need to tokenize it accordingly to the model we are going to use.\n",
    "\n",
    "In this case, as it is a BERT-like model, the tokenizer splits in subwords such as \"united\" -> \"unit\" + \"##ed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer is a Fast one [Table of Fast Tokenizers](https://huggingface.co/transformers/#supported-frameworks) and in depth explanation in [Chapter 6](https://huggingface.co/learn/nlp-course/en/chapter6/3). \n",
    "\n",
    "Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from — a feature we call offset mapping. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it’s inside, and vice versa.\n",
    "\n",
    "Special methods:\n",
    "- `tokenizer.is_fast` to check if the tokenizer is fast or not.\n",
    "- `encoding.tokens()` to convert ids to tokens.\n",
    "```python\n",
    "encoding = tokenizer(example)\n",
    "print(encoding.tokens())\n",
    "```\n",
    "\n",
    "```python\n",
    "['\\[CLS\\]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',  'Brooklyn', '.', '[SEP]']\n",
    "```\n",
    "- `encoding.word_ids()` to get the word ids, having None for special tokens and the same id for the rest of the tokens in the same word.\n",
    "```python\n",
    "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6]\n",
      "[19, 20, 21, 22, 23, 24, 24, 25, 26, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(sample[\"tokens\"], is_split_into_words=True).word_ids()[:10])\n",
    "print(tokenizer(sample[\"tokens\"], is_split_into_words=True).word_ids()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align tokens with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the new tokenizer splitted some of our words into sub-words. We need to align the labels with the new tokens.\n",
    "\n",
    "```\n",
    "Palacio [B-ADJP] -> ['Pa', '##la', '##cio'] [I-ADJP, I-ADJP, I-ADJP]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Palacio', 'had', 'earlier', 'accused', 'Fischler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"']\n",
      "['[CLS]', 'Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Pa', '##la', '##cio', 'had', 'earlier', 'accused', 'Fi', '##sch', '##ler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'un', '##ju', '##st', '##ified', 'alarm', 'through', '\"', 'dangerous', 'general', '##isation', '.', '\"', '[SEP]']\n",
      "[-100, 7, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 1, 2, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "['B-INTJ', 'O', 'O', 'B-ADJP', 'I-ADJP', 'I-ADJP', 'I-ADJP', 'I-ADJP', 'O', 'O', 'O', 'B-ADJP', 'I-ADJP', 'I-ADJP', 'O', 'O', 'B-ADVP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "27\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "new_labs = align_labels_with_tokens(sample[\"ner_tags\"], tokenizer(sample[\"tokens\"], is_split_into_words=True).word_ids())\n",
    "print(sample[\"tokens\"])\n",
    "print(tokenizer(sample[\"tokens\"], is_split_into_words=True).tokens())\n",
    "print(new_labs)\n",
    "print([raw_datasets[\"train\"].features[\"chunk_tags\"].feature.names[i] for i in new_labs[1:-1]])\n",
    "print(len(sample[\"tokens\"]))\n",
    "print(len(new_labs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this process is applied to all the examples in the dataset and so do the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the tokenizer is a fast one, it can be applied in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3250/3250 [00:00<00:00, 22460.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "token_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
